# 该项目的编写思路分享

项目基于 langchain 官方的`deepagents`进行改写，适配 qwen 和 deepseek 模型，采用 langgraph 构建。

## 图结构

如下图所示
<img src="./assets/graph.png">

## 图的状态构建

```python
class State(MessagesState, total=False):
    todo: list[Todo]
    task_messages: Annotated[list[AnyMessage], add_messages]
    note: Annotated[dict[str, str], file_reducer]
    now_task_message_index: int
    write_note_messages: Annotated[list[AnyMessage], add_messages]
```

这是本次的状态图的字典结构。
`todo`: 待办事项列表 <br>
`task_messages`: 子智能体执行的 message 列表 <br>
`note`: 笔记列表 <br>
`now_task_message_index`: 存储当前执行的子智能体的第一条 message 的索引 <br>
`write_note_messages`: 无特殊含义，仅用于存储笔记写入的工具调用和返回结果

## 智能体划分

整体来看分为三个智能体
`主智能体`：拆解用户需求，生成多个待办子任务，并调度子智能体执行
`子智能体`：执行单个待办子任务，并返回结果
`笔记智能体`：记录笔记，并返回结果

## 完整思路

1.首先获取用户输入，由主智能体根据用户的需求拆解任务分解成多个子任务。提供`write_todo`工具用于写入待办事项。注意这个就是和官方不同的地方，官方的这个工具即可以用于写入`todo`工具，也可以用于修改`todo`的完成状态，之所以这么做是因为我认为如果仅提供一个工具即用于写入最开始的 todo，同时后续修改也用这个，会造成 token 浪费，举一个例子，假设任务有[任务 1，任务 2，任务 3，任务 4]，那么第一次的时候写入所有的 todo list，后续更新的时候，比如任务 1 完成了，那么只需要修改任务 1 的状态为`done`和任务 2 的状态为`in_progres`，而任务 3 和任务 4 实际上是不需要修改的。这样会导致了 token 的浪费。<br> 2.完成 todo list 的写入后，会开始执行第一个任务，此时会使用`transfor_subagent`工具转交给 subagent 执行,subagent 会在单独的上下文窗口(代码中表现为`task_messages`)中进行，通过提示词给定 system prompt 为完成某一个子任务，同时后续的 message 只会是这个任务执行期间的 message，与前一个任务的 message 是隔离的<br>，这就是实现了上下文的隔离。<br> 3.给定 subagent 有多个工具可以使用（当然这里因为是测试目的，只写了一个工具`get_weather`用于获取天气，而且该函数没有实现），当完成后会将最后一条 AI 的 message 传递给笔记智能体<br> 4.之所以要有笔记智能体的目的是因为每个任务可能返回的内容是比较长的，如果只是用 ToolMessage 返回给主智能体会造成上下文太长，因此采用写出上下文的方式将内容保存到笔记中（抽象为 langgraph 的状态，和官方一样）<br> 5.对于笔记内容，无论是子智能体还是主智能体都可以查询，但是不同的是主智能体会根据当前任务完成后返回的 tool_message 来获取笔记的名称，而子智能体则是采用暴力的直接拼接上下文的方式，原因如下：对于主智能体，其的主要作用是分解任务和分配任务，其实他不一定要完全知道任务的所有结果，而且如果暴力的拼接到上下文中会造成 KV Cache 失效的问题。而对于子智能体，我们给它目前的笔记列表，是希望它如果当前任务需要先前的任务的结果，则可以按需查询，同时因为每个任务的执行过程都对应了一个单独的上下文窗口因此尽管是暴力的拼接也不会导致 KVCache 失效。<br> (当然这里需要说明的是，最开始我的目的是让主智能体按需的调用`query_note`方法，但是最终的情况是 deepseek 每一次执行完都会查看，这里可能是提示词没有表达清楚的问题。而对于子智能体，则是先告诉它目前有的笔记的名称，让它根据当前任务需要按需查询`query_note`) <br> 6.对于笔记的写入，因为写入的内容是比较长的（笔记的写入也是大模型工具调用，原有是最后一条 message 可能会包含一些没用的内容，所以需要大模型根据返回内容进行提取）因此采用了比较便宜的 qwen-flash 模型，同时这个模型速度非常快，工具调用也比较出色，当然了为了保证每次都能写入笔记，这里采用了强制工具调用的方式。

## 最后总结

当然这个项目编写的目的也只是个人觉得目前 langchain 的 deepagent 架构本身很不错，但是缺少一些比较重要的内容那比如上下文如何隔离，同时还有一个目的就是测试国内的模型的工具调用能力，根据这个项目的运行结果来看`deepseek-v3.1`和`qwen3-235b-a22b-instruct-2507`都具备较强的工具调用能力，但是`deepseek`模型更强一点，原因是因为它能稳定的进行上十次的工具调用，且每一次工具调用都较为准确，更难能可贵的是，有一次它发现子智能体执行的结果不是很满意，它还是重新调用再次执行。
![alt text](./assets/image.png)
